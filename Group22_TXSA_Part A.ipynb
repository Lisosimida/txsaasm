{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adamc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adamc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Classification is the task of choosing the correct class label for a given input. In basic\\nclassification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance. The basic classification task has a number of interesting variants. For example, in multiclass classification, each instance may be assigned multiple labels; in open-class classification, the set of labels is not defined in advance; and in sequence classification, a list of inputs are jointly classified.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileHandler = open('Data_1.txt')\n",
    "sampleText = fileHandler.read()\n",
    "sampleText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tokens = sampleText.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tokens = re.findall(r'\\b\\w+\\b', sampleText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Pakages Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens = word_tokenize(sampleText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(Tokens):\n",
    "    Tokens = [token.lower() for token in Tokens]\n",
    "    stopTokens = stopwords.words(\"english\") + list(string.punctuation)\n",
    "    filteredTokens = []\n",
    "\n",
    "    for w in Tokens:\n",
    "        if w not in stopTokens :\n",
    "            filteredTokens.append(w)\n",
    "    \n",
    "    return filteredTokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stop_words(Tokens):\n",
    "    Tokens = [token.lower() for token in Tokens]\n",
    "    stopTokens = stopwords.words(\"english\") + list(string.punctuation)\n",
    "    stopWords = []\n",
    "\n",
    "    for w in Tokens:\n",
    "        if w in stopTokens:\n",
    "            stopWords.append(w)\n",
    "    \n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_filtered = filter_tokens(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Split Tokenization:\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input.', 'In', 'basic\\nclassification', 'tasks,', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs,', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants.', 'For', 'example,', 'in', 'multiclass', 'classification,', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels;', 'in', 'open-class', 'classification,', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance;', 'and', 'in', 'sequence', 'classification,', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified.']\n",
      "\n",
      "2. Regular Expression Tokenization:\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', 'In', 'basic', 'classification', 'tasks', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', 'For', 'example', 'in', 'multiclass', 'classification', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', 'in', 'open', 'class', 'classification', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', 'and', 'in', 'sequence', 'classification', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified']\n",
      "\n",
      "3. NLTK Tokenization:\n",
      "['Classification', 'is', 'the', 'task', 'of', 'choosing', 'the', 'correct', 'class', 'label', 'for', 'a', 'given', 'input', '.', 'In', 'basic', 'classification', 'tasks', ',', 'each', 'input', 'is', 'considered', 'in', 'isolation', 'from', 'all', 'other', 'inputs', ',', 'and', 'the', 'set', 'of', 'labels', 'is', 'defined', 'in', 'advance', '.', 'The', 'basic', 'classification', 'task', 'has', 'a', 'number', 'of', 'interesting', 'variants', '.', 'For', 'example', ',', 'in', 'multiclass', 'classification', ',', 'each', 'instance', 'may', 'be', 'assigned', 'multiple', 'labels', ';', 'in', 'open-class', 'classification', ',', 'the', 'set', 'of', 'labels', 'is', 'not', 'defined', 'in', 'advance', ';', 'and', 'in', 'sequence', 'classification', ',', 'a', 'list', 'of', 'inputs', 'are', 'jointly', 'classified', '.']\n",
      "\n",
      "Filtered NLTK Tokenization:\n",
      "['classification', 'task', 'choosing', 'correct', 'class', 'label', 'given', 'input', 'basic', 'classification', 'tasks', 'input', 'considered', 'isolation', 'inputs', 'set', 'labels', 'defined', 'advance', 'basic', 'classification', 'task', 'number', 'interesting', 'variants', 'example', 'multiclass', 'classification', 'instance', 'may', 'assigned', 'multiple', 'labels', 'open-class', 'classification', 'set', 'labels', 'defined', 'advance', 'sequence', 'classification', 'list', 'inputs', 'jointly', 'classified']\n",
      "\n",
      "Stop Words Found in the Text:\n",
      "['is', 'the', 'of', 'the', 'for', 'a', '.', 'in', ',', 'each', 'is', 'in', 'from', 'all', 'other', ',', 'and', 'the', 'of', 'is', 'in', '.', 'the', 'has', 'a', 'of', '.', 'for', ',', 'in', ',', 'each', 'be', ';', 'in', ',', 'the', 'of', 'is', 'not', 'in', ';', 'and', 'in', ',', 'a', 'of', 'are', '.']\n",
      "\n",
      "There are 49 stop words in the text.\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Split Tokenization:\")\n",
    "print(split_tokens)\n",
    "\n",
    "print(\"\\n2. Regular Expression Tokenization:\")\n",
    "print(re_tokens)\n",
    "\n",
    "print(\"\\n3. NLTK Tokenization:\")\n",
    "print(nltk_tokens)\n",
    "print(\"\\nFiltered NLTK Tokenization:\")\n",
    "print(nltk_filtered)\n",
    "\n",
    "print(\"\\nStop Words Found in the Text:\")\n",
    "stop_words_found = find_stop_words(nltk_tokens)\n",
    "print(stop_words_found)\n",
    "\n",
    "print(\"\\nThere are \" + str(len(stop_words_found)) + \" stop words in the text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classification', 'task', 'choos', 'correct', 'clas', 'label', 'given', 'input', 'basic', 'classification', 'task', 'input', 'consider', 'isolation', 'input', 'set', 'label', 'defin', 'advance', 'basic', 'classification', 'task', 'number', 'interest', 'variant', 'example', 'multiclas', 'classification', 'instance', 'may', 'assign', 'multiple', 'label', 'open-clas', 'classification', 'set', 'label', 'defin', 'advance', 'sequence', 'classification', 'list', 'input', 'joint', 'classifi']\n",
      "\n",
      " total number of words after stemming:  45\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def stem_word(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "print([stem_word(t) for t in nltk_filtered])\n",
    "print(\"\\n total number of words after stemming: \", len(nltk_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classif', 'task', 'choos', 'correct', 'class', 'label', 'given', 'input', 'basic', 'classif', 'task', 'input', 'consid', 'isol', 'input', 'set', 'label', 'defin', 'advanc', 'basic', 'classif', 'task', 'number', 'interest', 'variant', 'exampl', 'multiclass', 'classif', 'instanc', 'may', 'assign', 'multipl', 'label', 'open-class', 'classif', 'set', 'label', 'defin', 'advanc', 'sequenc', 'classif', 'list', 'input', 'jointli', 'classifi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pstem = PorterStemmer()\n",
    "\n",
    "pstem_tokens = [pstem.stem(t) for t in nltk_filtered]\n",
    "\n",
    "print(pstem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'task', 'choos', 'correct', 'class', 'label', 'giv', 'input', 'bas', 'class', 'task', 'input', 'consid', 'isol', 'input', 'set', 'label', 'defin', 'adv', 'bas', 'class', 'task', 'numb', 'interest', 'vary', 'exampl', 'multiclass', 'class', 'inst', 'may', 'assign', 'multipl', 'label', 'open-class', 'class', 'set', 'label', 'defin', 'adv', 'sequ', 'class', 'list', 'input', 'joint', 'class']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lstem = LancasterStemmer()\n",
    "\n",
    "lstem_tokens = [lstem.stem(t) for t in nltk_filtered]\n",
    "\n",
    "print(lstem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 POS Tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The big black dog barked at the white cat and chased away.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileHandler = open('Data_2.txt')\n",
    "text = fileHandler.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk_tokens = word_tokenize(text)\n",
    "nltk_pos_tags = nltk.pos_tag(nltk_tokens)\n",
    "print(nltk_pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textblob POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('big', 'JJ'), ('black', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('white', 'JJ'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "wiki = TextBlob(text)\n",
    "print(wiki.tags)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NN'), ('big', 'NN'), ('black', 'NN'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'NN'), ('the', 'NN'), ('white', 'NN'), ('cat', 'NN'), ('and', 'NN'), ('chased', 'VBD'), ('away', 'NN'), ('.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "patterns = [\n",
    "     (r'.*ing$', 'VBG'),               # gerunds\n",
    "     (r'.*ed$', 'VBD'),                # simple past\n",
    "     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "     (r'.*ould$', 'MD'),               # modals\n",
    "     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "     (r'.*s$', 'NNS'),                 # plural nouns\n",
    "     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "     (r'.*', 'NN'),                    # nouns (default)\n",
    "     (r'^\\d+$', 'CD'),\n",
    "     (r'.*ing$', 'VBG'),               # gerunds, i.e. wondering\n",
    "     (r'.*ment$', 'NN'),               # i.e. wonderment\n",
    "     (r'.*ful$', 'JJ')                 # i.e. wonderful\n",
    " ]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "print(regexp_tagger.tag(nltk_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import CFG\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP PERIOD\n",
    "    NP -> Det Adj N | Det Adj Adj N | Det N | NP Conj NP\n",
    "    VP -> V NP | V PP | VP Conj VP | V Adv\n",
    "    PP -> P NP\n",
    "    Det -> 'The' | 'the'\n",
    "    Adj -> 'big' | 'black' | 'white'\n",
    "    N -> 'dog' | 'cat'\n",
    "    V -> 'barked' | 'chased'\n",
    "    P -> 'at'\n",
    "    Conj -> 'and'\n",
    "    Adv -> 'away'\n",
    "    PERIOD -> '.'\n",
    "\"\"\")\n",
    "\n",
    "# Parse the sentence\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "# Display the parse trees\n",
    "for tree in parser.parse(nltk_tokens):\n",
    "    tree.draw()  # Open GUI-based tree visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Sentence Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('read', 'a'): 3,\n",
       "         ('<s>', 'He'): 2,\n",
       "         ('He', 'read'): 2,\n",
       "         ('a', 'book'): 2,\n",
       "         ('book', '</s>'): 2,\n",
       "         ('</s>', '<s>'): 2,\n",
       "         ('<s>', 'I'): 1,\n",
       "         ('I', 'read'): 1,\n",
       "         ('a', 'different'): 1,\n",
       "         ('different', 'book'): 1,\n",
       "         ('book', 'by'): 1,\n",
       "         ('by', 'Danielle'): 1,\n",
       "         ('Danielle', '</s>'): 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "training_corpus = [\n",
    "    \"<s> He read a book </s>\",\n",
    "    \"<s> I read a different book </s>\",\n",
    "    \"<s> He read a book by Danielle </s>\",\n",
    "]\n",
    "\n",
    "fileHandler = open('Data_3.txt')\n",
    "target_sentence = fileHandler.read()\n",
    "\n",
    "tokens = [token for sentence in training_corpus for token in sentence.split()]\n",
    "\n",
    "def bigrams(tokens):\n",
    "    return [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
    "\n",
    "bigram_count = Counter(bigrams(tokens))  \n",
    "bigram_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsmoothed Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of sentence'Training Corpus\n",
      "~~~~~~~~~~~~~\n",
      "<s> He read a book </s>\n",
      "<s> I read a different book </s>\n",
      "<s> He read a book by Danielle </s>\n",
      "\n",
      "Calculate sentence probability for the following sentence\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "<s> I read a book by Danielle </s>' using unsmoothed bigram model is 0.00000\n"
     ]
    }
   ],
   "source": [
    "#Function to calculate the smoothed probability\n",
    "def unsmoothed_probability(sentence):\n",
    "    sentence_tokens = sentence.split()              #tokenize the sentence\n",
    "    sentence_bigrams = bigrams(sentence_tokens)     #generate bigrams from sentence\n",
    "    probability = 1.0                               #initialize probability to 1\n",
    "\n",
    "    for bigram in sentence_bigrams:\n",
    "\n",
    "        count_bigram = bigram_count.get(bigram,0)   #get count of bigram\n",
    "        count_unigram = tokens.count(bigram[0])     #get count of unigram\n",
    "        if count_unigram == 0:\n",
    "            probability = 0                         #unseen unigram\n",
    "        else:\n",
    "            probability *= count_bigram / count_unigram\n",
    "\n",
    "    return probability\n",
    "\n",
    "#calculate unsmoothed probability of the target sentence\n",
    "probability = unsmoothed_probability(target_sentence)\n",
    "print(f\"The probability of sentence'{target_sentence}' using unsmoothed bigram model is {probability:.5f}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothed Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of sentence'Training Corpus\n",
      "~~~~~~~~~~~~~\n",
      "<s> He read a book </s>\n",
      "<s> I read a different book </s>\n",
      "<s> He read a book by Danielle </s>\n",
      "\n",
      "Calculate sentence probability for the following sentence\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "<s> I read a book by Danielle </s>' using smoothed bigram model is 0.000000000\n"
     ]
    }
   ],
   "source": [
    "#Define Vocabularly Size\n",
    "v_size = len(set(tokens))\n",
    "\n",
    "#Function to calculate the smoothed probability\n",
    "def smoothed_probability(sentence):\n",
    "    sentence_tokens = sentence.split()\n",
    "    sentence_bigrams = bigrams(sentence_tokens)\n",
    "    probability = 1.0\n",
    "\n",
    "    for bigram in sentence_bigrams:\n",
    "        count_bigram = bigram_count.get(bigram,0)                       #get the count of bigram\n",
    "        count_unigram = tokens.count(bigram[0])                         #get the count of unigram\n",
    "        probability *= (count_bigram + 1) / (count_unigram + v_size)    #add 1 smoothing\n",
    "    return probability\n",
    "\n",
    "#calculate smoothed probabiltiy of target sentence\n",
    "probability = smoothed_probability(target_sentence)\n",
    "print(f\"The probability of sentence'{target_sentence}' using smoothed bigram model is {probability:.9f}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
